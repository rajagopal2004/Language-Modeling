# Language-Modeling

Skip-gram and CBOW Models with LSTM-based Next Word Prediction
Overview
This repository implements Skip-gram and Continuous Bag of Words (CBOW) models for generating word embeddings. Additionally, it includes an RNN-based LSTM model for next-word prediction in a sequence.

# Models
1. Skip-gram Model
Purpose: Predicts context words given a target word.
Objective: Maximize the probability of context words around a target word.
2. CBOW Model
Purpose: Predicts a target word given surrounding context words.
Objective: Maximize the probability of the target word given its context.
3. LSTM for Next Word Prediction
Purpose: Predicts the next word in a sequence using an RNN-based LSTM architecture.
Use Case: Language modeling and text generation.
# Usage
Training: Run the provided scripts to train the models on your dataset.
Prediction: Use the trained models to generate word embeddings or predict the next word.
